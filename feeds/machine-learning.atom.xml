<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Scaling Solutions</title><link href="http://scalingsolutions.github.io/" rel="alternate"></link><link href="/feeds/machine-learning.atom.xml" rel="self"></link><id>http://scalingsolutions.github.io/</id><updated>2014-04-27T22:50:00-07:00</updated><entry><title>Intuition for Support Vector Regression and Gaussian Processess</title><link href="http://scalingsolutions.github.io/blog/2014/04/27/intuition-for-SVR/" rel="alternate"></link><updated>2014-04-27T22:50:00-07:00</updated><author><name>ScalingSolutions</name></author><id>tag:scalingsolutions.github.io,2014-04-27:blog/2014/04/27/intuition-for-SVR/</id><summary type="html">&lt;p&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;Support vector regression (SVR) is a fast and accurate way of interpolating your data. It is useful when you have an expensive function you want to approximate over a known domain. It learns quickly and is systematically improvable. Variants of SVR are used throughout science including Krigging and Gaussian processes. In this short post I'll explain how you can build and train a SVR machine, focusing mostly on the Gaussian process variety, and talk about some circumstances where it is useful.&lt;/p&gt;
&lt;h1 id="what-is-svr"&gt;What is SVR?&lt;/h1&gt;
&lt;p&gt;To build a SVR you must perform 4 tasks,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt; 
Collect a training set, &lt;span class="math"&gt;\(\vec{X}, \vec{Y}\)&lt;/span&gt;.
&lt;/li&gt;
&lt;li&gt; 
Choose a kernel and it's parameters as well as any regularization you may need.
&lt;/li&gt;
&lt;li&gt; 
Form the correlation matrix, &lt;span class="math"&gt;\(\bar{K}\)&lt;/span&gt;.
&lt;/li&gt;
&lt;li&gt; 
Solve a linear equation, exactly or approximately, to get contraction coefficients, &lt;span class="math"&gt;\(\vec{\alpha} = \{ \alpha_i \}\)&lt;/span&gt;.
&lt;/li&gt;
&lt;li&gt; 
Using those coefficients, create your estimator, &lt;span class="math"&gt;\(f(\vec{X},\vec{\alpha},x^\star) = y^\star\)&lt;/span&gt;.
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now let's take each of these steps one by one and talk a little bit about what they are doing.&lt;/p&gt;
&lt;h2 id="the-training-set"&gt;The training set&lt;/h2&gt;
&lt;p&gt;Your training set consists of the samples you've collected to train your machine. They should span the domain you are expecting to evaluate later. For instance, if you are using your SVR machine to estimate the cosine function, you should train it on the domain &lt;span class="math"&gt;\(0 \rightarrow 2\pi\)&lt;/span&gt; (and shift your evaluations into that range). When you evaluate the SVR machine outside the domain you evaluate the result will vary depending on your choice of kernel and optimization routine.&lt;/p&gt;
&lt;h2 id="choosing-a-kernel"&gt;Choosing a kernel&lt;/h2&gt;
&lt;p&gt;You are free to choose whatever kernel you would like. In this example I'll focus on using the Gaussian kernel, &lt;span class="math"&gt;\[ K_G(\vec{x^i},\vec{x^j},\vec{\theta}) = \exp{\left( \sum_k^{N_{D}} \theta_k\left|x_k^i - x_k^j\right|^2  \right)}\]&lt;/span&gt; where &lt;span class="math"&gt;\(N_D\)&lt;/span&gt; is the number of dimensions in each data point &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; and the vector &lt;span class="math"&gt;\(\vec{\theta}\)&lt;/span&gt; is a set of hyperparameters. Some other common choices of kernel are polynomial, Laplacian, sigmoid, and radial basis functions. All of these functions have a set of hyperparameters that must be trained.&lt;/p&gt;
&lt;p&gt;In the gaussian process framework, the statistical interpretation of these parameters are somewhat complicated, but totally worth checking out. I refer you to one of my favorite papers on the subject, &lt;a href="http://scitation.aip.org/content/aip/journal/jcp/136/7/10.1063/1.3684884"&gt;&amp;quot;Bi-fidelity fitting and optimization&amp;quot;&lt;/a&gt;, if you can get behind the paywall. Otherwise a pretty decent source can be found at &lt;a href="http://www.gaussianprocess.org/gpml/chapters/"&gt;&amp;quot;Gaussian Processes for Machine Learning&amp;quot;&lt;/a&gt;. In short, they are chosen such that they maximize the marginal probability of the training set being generated using the kernel as the Bayesian prior.&lt;/p&gt;
&lt;p&gt;This training is the most expensive part of performing SVR, and much research has gone into developing good ways to do it. We can train it straightforwardly, if expensively, using CG or another gradient based optimization method and minimizing the cost function. The explicit form of the cost function can be found in equation 5.8 in &lt;a href="http://www.gaussianprocess.org/gpml/chapters/"&gt;&amp;quot;Gaussian Processes for Machine Learning&amp;quot;&lt;/a&gt;. Other forms of SVM will use different cost functions and parameter selection routines.&lt;/p&gt;
&lt;h3 id="does-the-kernel-matter"&gt;Does the kernel matter?&lt;/h3&gt;
&lt;p&gt;Yes! If you choose a kernel that goes to zero as the distance between it's arguments grows, such as the Gaussian above, &lt;span class="math"&gt;\(K_G(0,\infty)=0\)&lt;/span&gt;, then as you move away from your training data, the machine will return the mean value of your training set. If you choose a function that grows larger, such as linear or polynomial, the value will also grow linearly or polynomially. Your choice of kernel will determine the asymptotic behavior of your estimator. Choose your kernel appropriately for your problem.&lt;/p&gt;
&lt;h3 id="does-the-regularization-matter"&gt;Does the regularization matter?&lt;/h3&gt;
&lt;p&gt;Yes! For training sets with some noise, the regularizer helps prevent wild fluctuations between data points by smoothing out the prior. You can think of this process as adding some slack to the machine. Regularization is achieved by adding a small positive component to the diagonal of the correlation matrix.&lt;/p&gt;
&lt;h2 id="forming-the-correlation-matrix"&gt;Forming the correlation matrix&lt;/h2&gt;
&lt;p&gt;Forming this matrix is straightforward and really doesn't require any discussion. You evaluate your kernel for all pairs of points in the training set and then add your regularizer to the diagonal. This results in the matrix, &lt;span class="math"&gt;\[ K_{i,j} = \exp{\left( \sum_k \theta_k\left|x_k^i - x_k^j\right|^2  \right)} + \epsilon \delta_{i,j}\]&lt;/span&gt; where &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is your regularizer and &lt;span class="math"&gt;\(\delta_{i,j}\)&lt;/span&gt; is the kroeniker delta function. This matrix is symmetric positive definite (semi-definite in the numerical sense) and easy to work with. It represents the correlation matrix of the data in a &lt;a href="http://www.cs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec3.pdf"&gt;higher dimensional space&lt;/a&gt; than the one your training set is drawn from.&lt;/p&gt;
&lt;h2 id="solve-the-matrix-to-form-your-estimator"&gt;Solve the matrix to form your estimator&lt;/h2&gt;
&lt;p&gt;This is the meat of the algorithm. Fortunately, it's all just linear algebra. Your machine works like this, &lt;span class="math"&gt;\[ \bar{K} \vec{\alpha} = \vec{y} \]&lt;/span&gt; where &lt;span class="math"&gt;\(\vec{y}\)&lt;/span&gt; is the vector of values corresponding to your training set, &lt;span class="math"&gt;\(\bar{K}\)&lt;/span&gt; is still your correlation matrix, and &lt;span class="math"&gt;\(\vec{\alpha}\)&lt;/span&gt; is a set of unknowns we need to solve for. Doing so is simple, we just invert the matrix &lt;span class="math"&gt;\(\bar{K}\)&lt;/span&gt; and apply it to the vector, &lt;span class="math"&gt;\(\vec{y}\)&lt;/span&gt;, &lt;span class="math"&gt;\[  \vec{\alpha} = \bar{K}^{-1}\vec{y}. \]&lt;/span&gt; Because our correlation matrix is so well behaved, efficient methods for inversion can be used (QR/Cholesky).&lt;/p&gt;
&lt;h2 id="forming-the-estimator"&gt;Forming the estimator&lt;/h2&gt;
&lt;p&gt;Once your &lt;span class="math"&gt;\(\vec{\alpha}\)&lt;/span&gt; parameters are known, forming the estimator is straightforward. To estimate the value, &lt;span class="math"&gt;\(y^\star\)&lt;/span&gt;, for a test point, &lt;span class="math"&gt;\(\vec{x}^\star\)&lt;/span&gt;, we simply compute the correlation vector, &lt;span class="math"&gt;\(\vec{k}\)&lt;/span&gt;, and find the inner product with the alpha vector, &lt;span class="math"&gt;\[ y^\star = \vec{\alpha}\cdot\vec{k} \]&lt;/span&gt; where we compute the elements of &lt;span class="math"&gt;\(\vec{k}\)&lt;/span&gt; as, &lt;span class="math"&gt;\[ k_i = \exp{\left( \sum_k \theta_k\left|x_k^i - x_k^\star\right|^2  \right)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You can see from the previous equation, if our regularizer is zero and we've inverted our matrix exactly, then the estimator perfectly passes though all of our training points. That is because the vector &lt;span class="math"&gt;\(\vec{k}\)&lt;/span&gt; is identical to a row in the correlation matrix if the test point is in the training set! Once the regularizer is included or we have gotten our parameters using something quick but approximate, we don't recover our training set solutions, but htey should be close.&lt;/p&gt;
&lt;h3 id="dirty-tricks"&gt;Dirty tricks&lt;/h3&gt;
&lt;p&gt;You may notice that I have implicitly removed the mean value from my training set data. You add the mean value from the training set back into the estimator when you're done, &lt;span class="math"&gt;\[ y^\star = \vec{\alpha}\cdot\vec{k} + \mu \]&lt;/span&gt; where &lt;span class="math"&gt;\(\mu = \bar{y}\)&lt;/span&gt; and all &lt;span class="math"&gt;\(y\)&lt;/span&gt; are recentered, &lt;span class="math"&gt;\(y\rightarrow y-\bar{y}\)&lt;/span&gt;. This makes it easier for the algorithm to interpolate the differences between the data points in your trinaing set instead of working very hard to also reproduce the mean.&lt;/p&gt;
&lt;p&gt;In practice, you also want to scale your inputs so they all have the same range. The reason for this trick is the same as for any other machine that requires some nonlinear optimization. You're trying to make the minimum for your cost function as smooth and harmonic looking as possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="what-is-the-svr-really"&gt;What is the SVR &lt;em&gt;really&lt;/em&gt;?&lt;/h1&gt;
&lt;p&gt;So, now that I've taken all this time to tell you how a SVR machine, and in particular Gaussian process machine, is built, I owe you an intuitive explanation of what it is doing. It's not an easy task, so bear with me for a bit and I hope it will pay off.&lt;/p&gt;
&lt;p&gt;The SVR performs linear regression in a higher (infinite) dimensional space. A simple way to think of it is as if each data point in your training set represents it's own dimension. When you evaluate your kernel between a test point and a point in your training set, the resulting value gives you the coordinate of your test point in that dimension. The vector we get when we evaluate the test point for all points in the training set, &lt;span class="math"&gt;\(\vec{k}\)&lt;/span&gt;, is the representation of the test point in the higher dimensional space. The form of the kernel tells you about the geometry of that higher dimensional space.&lt;/p&gt;
&lt;p&gt;Once you have that vector, you use it to perform a linear regression. You can tell it is a linear regression because of the form of the estimator, it's an inner product! Thus the intuition for the machine is rather simple, though the procedure and parameters may be difficult to interpret.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="so-whats-the-svr-good-for"&gt;So, what's the SVR good for?&lt;/h1&gt;
&lt;p&gt;Because you are able to generate training points, you know what the &amp;quot;right&amp;quot; answer is. However, it may be very expensive to compute that answer for every new point you need. The SVR, and in particular Gaussian processes, are very good at providing a cheap surrogate to an expensive call. If the function you are trying to compute is smooth, and you expect to have to call it over and over again, you may be able to gain a significant savings by pre-computing a training set and then using a SVR machine to interpolate the results.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="further-reading"&gt;Further reading&lt;/h1&gt;
&lt;p&gt;For a more extensive discussion and tutorial on SVR you should check out Smola and Scholkopf's excellent review from several years back, &amp;quot;&lt;a href="http://alex.smola.org/papers/2003/SmoSch03b.pdf"&gt;A Tutorial on Support Vector Regression&lt;/a&gt;.&amp;quot; You can also find more detail in texts such as &amp;quot;&lt;a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/"&gt;The Elements of Statistical Learning&lt;/a&gt;.&amp;quot;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="next-time"&gt;Next time&lt;/h1&gt;
&lt;p&gt;At this point we'll split the blog postings up. I'll talk a little about the parameters and give an example. Ben will put things into context by talking about some of the different SVR machines and related pieces. John will take my learning speed and benchmark it against a much faster version built using continuum's tools.&lt;/p&gt;
&lt;p&gt;Drop Jeremy a line at &lt;a href="http://www.quora.com/Jeremy-McMinis"&gt;Quora&lt;/a&gt; or &lt;a href="https://github.com/mcminis1"&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;/p&gt;</summary></entry></feed>